---
title: "101C Final Project"
author: "Daren Sathasivam, Kirtan Bhatt, Derek Diaz, Michael Gureghian"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## From Homework 4

```{r}
set.seed(1)

#########################
# --- Data Cleaning --- #
#########################

# Datasets
obesity_train <- read.csv("ObesityTrain2.csv") # ObStatus
obesity_test <- read.csv("ObesityTestNoY2.csv")
# head(obesity_train, n = 2)

# Function to identify columns except target var
identify_columns <- function(data, exclude_col = NULL) {
  numeric_cols <- sapply(data, is.numeric)
  categorical_cols <- sapply(data, function(x) is.factor(x) || is.character(x))
  # Assign numeric or categorical
  if (!is.null(exclude_col) && exclude_col %in% names(data)) {
    numeric_cols <- numeric_cols[-which(names(data) == exclude_col)]
    categorical_cols <- categorical_cols[-which(names(data) == exclude_col)]
  }
  list(numeric = numeric_cols, categorical = categorical_cols)
}
# To exclude target var in training
cols_train <- identify_columns(obesity_train, "ObStatus")
cols_test <- identify_columns(obesity_test)

# Function to impute missing values
# Median for numeric --- Most frequent for categorical
impute_median_mode <- function(data, numeric_cols, categorical_cols) {
  # Num stuff
  for (col in names(data)[numeric_cols]) {
    if (any(is.na(data[[col]]))) {
      # insert median at numeric NA
      data[[col]][is.na(data[[col]])] <- median(data[[col]], na.rm = TRUE)
    }
  }
  # Cat stuff
  for (col in names(data)[categorical_cols]) {
    if (any(is.na(data[[col]]))) {
      # insert mode at categorical NA
      mode_value <- names(which.max(table(data[[col]])))
      data[[col]][is.na(data[[col]])] <- mode_value
    }
  }
  data[categorical_cols] <- lapply(data[categorical_cols], factor) # factor
  return(data)
}
obesity_train_imputed <- impute_median_mode(obesity_train, cols_train$numeric, cols_train$categorical)
obesity_test_imputed <- impute_median_mode(obesity_test, cols_test$numeric, cols_test$categorical)

Y_train <- obesity_train_imputed$O.train.C...30.
X_train <- obesity_train_imputed[, -which(names(obesity_train_imputed) == "ObStatus")]

# Check if there are any remaining NAs in the training and test datasets
cat("Remaining NAs in Training Data:\n")
print(colSums(is.na(obesity_train_imputed)))
cat("Remaining NAs in Testing Data:\n")
print(colSums(is.na(obesity_test_imputed)))





##############################
# --- Variable Selection --- #
##############################

str(obesity_train_imputed)
# Factor so can be the 'y' in models
obesity_train_imputed$ObStatus <- as.factor(obesity_train_imputed$ObStatus)
sum(is.na(obesity_train_imputed$ObStatus)) # to check for NAs
# Convert factors into dummy variables
obesity_train_encoded <- model.matrix(ObStatus ~ ., data = obesity_train_imputed)[, -1]  # Drop the intercept column
# Convert back to a data frame
obesity_train_encoded <- as.data.frame(obesity_train_encoded)
# Add the numeric target variable
obesity_train_encoded$ObStatus_num <- as.numeric(obesity_train_imputed$ObStatus)


# --- VIF > 5 indicates multicollinearity --- #
# Fit the linear model
full_model <- lm(ObStatus_num ~ ., data = obesity_train_encoded)
# Check the model summary
summary(full_model)
# Calculate VIF
library(car)
vif_values <- vif(full_model)
vif_sorted <- sort(vif_values, decreasing = TRUE)
print(vif_sorted)
# Identify high VIF variables
high_vif_vars <- names(vif_sorted[vif_sorted > 5])
cat("Variables with high VIF (> 5):", high_vif_vars, "\n")

# --- Exclude High VIF Variables from ^ --- #
high_vif_vars <- c("CALCSometimes", "CALCno", "CALCFrequently", 
                   "work_typePrivate", "work_typeGovt_job", 
                   "work_typeSelf-employed", "work_typeNever_worked")
obesity_train_no_multicollinearity <- obesity_train_encoded[, !names(obesity_train_encoded) %in% high_vif_vars]
reduced_model <- lm(ObStatus_num ~ ., data = obesity_train_no_multicollinearity)
vif_reduced <- vif(reduced_model)
vif_reduced_sorted <- sort(vif_reduced, decreasing = TRUE)
print(vif_reduced_sorted)
remaining_high_vif_vars <- names(vif_reduced_sorted[vif_reduced_sorted > 5])
cat("Remaining variables with high VIF (> 5):", remaining_high_vif_vars, "\n") # None have high VIF now
summary(reduced_model)




# --- Correlation Matrix (> 0.7 or < -0.7:: Moderate Correlation) --- #
# Compute correlation matrix for numeric predictors
numeric_vars <- names(obesity_train_imputed)[sapply(obesity_train_imputed, is.numeric)]
cor_matrix <- cor(obesity_train_imputed[, numeric_vars], use = "complete.obs")
cor_matrix
highest_correlation <- max(abs(cor_matrix[lower.tri(cor_matrix)]))
highest_correlation
# Find highly correlated pairs and exclude self-correlations
high_corr <- which(abs(cor_matrix) > 0.7, arr.ind = TRUE)
high_corr <- high_corr[high_corr[, 1] != high_corr[, 2], ]  
nrow(high_corr) # None to display

head(obesity_train_imputed, n = 2)

# Stepwise using AIC
library(MASS)
stepwise_ob_model <- stepAIC(glm(ObStatus ~ ., data = obesity_train_imputed, family = binomial), direction = "both")
summary(stepwise_ob_model)
selected_vars <- names(coef(stepwise_ob_model))[-1]
cat("Variables selected by stepwise AIC with logistic regression:", selected_vars, "\n")


# Variable Importance using Random Forest
library(randomForest)
rf_model <- randomForest(ObStatus ~ ., data = obesity_train_imputed, importance = TRUE)
var_importance <- importance(rf_model)
var_importance_sorted <- var_importance[order(var_importance[, 1], decreasing = TRUE), ]
print(var_importance_sorted)
top_vars <- rownames(var_importance_sorted)[1:17]  # Keep top 17 predictors
cat("Top predictors based on random forest importance:", top_vars, "\n")

# After taking into consideration the VIF, Stepwise, and RF Importance
selected_numeric <- c("Height", "Age", "FAF", "NCP", "CH2O", "Cholesterol", "avg_glucose_level") # Reduced from 11 to 7
selected_categorical <- c("Race", "MTRANS", "CAEC", "family_history_with_overweight", "FAVC", "Gender", "SCC", "ever_married") # Reduced from 18 to 8
selected_predictors <- c(selected_numeric, selected_categorical)



####################
# --- Modeling --- #
####################


# Scale for KNN --- Week-4 Discussion example for train() and preProcess
numeric_cols <- sapply(obesity_train_imputed, is.numeric)
preProcValues <- preProcess(obesity_train_imputed[, numeric_cols], method = c("center", "scale"))
X_train_scaled <- predict(preProcValues, obesity_train_imputed[, numeric_cols])
X_test_scaled <- predict(preProcValues, obesity_test_imputed[, numeric_cols])
obesity_train_imputed[, numeric_cols] <- X_train_scaled
obesity_test_imputed[, numeric_cols] <- X_test_scaled

# --- Model ---
train_control <- trainControl(method = "cv", number = 10, savePredictions = TRUE)
knn_model <- train(ObStatus ~ ., data = obesity_train_imputed, method = "knn", tuneGrid = data.frame(k = 5), trControl = train_control)

# Predictions
predictions_knn <- predict(knn_model, newdata = obesity_test_imputed)
length(predictions_knn)
# CSV to submit
results <- data.frame(ID = seq(1, length(predictions_knn)), ObStatus = predictions_knn) # won't let me submit without 'ID'
head(results)
dim(results)
# write.csv(results, "ObesitySol.csv", row.names = FALSE)

# dim(obesity_sample_sol)

# Summarize results
summary(knn_model)
knn_model$results
cat("KNN(k = 5) 10-fold Cross Validation accuracy: ", max(knn_model$results$Accuracy), ". \n")


### --- AFTER VARIABLE SELECTION --- ###

selected_predictors
obesity_train_final <- obesity_train_imputed[, c(selected_predictors, "ObStatus")]
obesity_test_final <- obesity_test_imputed[, selected_predictors]

# --- KNN --- #
numeric_cols <- sapply(obesity_train_final, is.numeric)
preProcValues <- preProcess(obesity_train_final[, numeric_cols], method = c("center", "scale"))
obesity_train_final[, numeric_cols] <- predict(preProcValues, obesity_train_final[, numeric_cols])
obesity_test_final[, numeric_cols] <- predict(preProcValues, obesity_test_final[, numeric_cols])

set.seed(1)
train_control <- trainControl(method = "cv", number = 10, savePredictions = TRUE)
knn_model <- train(ObStatus ~ ., data = obesity_train_final, method = "knn", tuneGrid = data.frame(k = seq(1, 15, by = 2)), trControl = train_control)

summary(knn_model)
knn_model$results
cat("KNN(k = 1 to 15 odd) 10-fold Cross Validation accuracy: ", max(knn_model$results$Accuracy), ". \n")
knn_predictions <- predict(knn_model, newdata = obesity_test_final)
length(knn_predictions)
knn_results <- data.frame(ID = seq(1, length(knn_predictions)), ObStatus = knn_predictions) # won't let me submit without 'ID'
head(knn_results)
dim(knn_results)
# write.csv(knn_results, "knn_ObesitySol.csv", row.names = FALSE)
knn_model2 <- train(ObStatus ~ ., data = obesity_train_final, method = "knn", tuneGrid = data.frame(k = 1), trControl = train_control)
summary(knn_model2)
knn_model2$results
cat("KNN(k = 1) 10-fold Cross Validation accuracy: ", max(knn_model2$results$Accuracy), ". \n")

knn_model3 <- train(ObStatus ~ ., data = obesity_train_final, method = "knn", tuneGrid = data.frame(k = 5), trControl = train_control)
summary(knn_model3)
knn_model3$results
cat("KNN(k = 5) 10-fold Cross Validation accuracy: ", max(knn_model3$results$Accuracy), ". \n")




# --- Random Forest --- #
# https://www.geeksforgeeks.org/random-forest-approach-in-r-programming/
# Also Chapter 8 Random forests/bagging
set.seed(1)
rf_model <- train(
  ObStatus ~ .,
  data = obesity_train_final,
  method = "rf",
  trControl = train_control
)

summary(rf_model)
rf_model$results
cat("Random Forest 10-fold CV Accuracy: ", max(rf_model$results$Accuracy), ".\n")

rf_predictions <- predict(rf_model, newdata = obesity_test_final)
rf_results <- data.frame(ID = seq(1, length(rf_predictions)), ObStatus = rf_predictions)
# write.csv(rf_results, "rf_ObesitySol.csv", row.names = FALSE)


# --- Attempt Boosting --- #
library(gbm)

```

#- -- MICE Impute Method --- #

# Try on original data with NA's 
impute_methods <- make.method(obesity_train)

# Assign methods for categorical variables
impute_methods["Race"] <- "polyreg"       # 4 levels
impute_methods["MTRANS"] <- "polyreg"    # 5 levels
impute_methods["CAEC"] <- "polyreg"      # 4 levels

impute_methods["family_history_with_overweight"] <- "logreg"  # 2 levels
impute_methods["FAVC"] <- "logreg"                            # 2 levels
impute_methods["Gender"] <- "logreg"                          # 2 levels
impute_methods["SCC"] <- "logreg"                             # 2 levels
impute_methods["ever_married"] <- "logreg"                    # 2 levels

# Assign 'pmm' for numeric predictors
impute_methods[selected_numeric] <- "pmm"

test_impute <- mice(obesity_train, m = 5, maxit = 50, method = impute_methods, seed = 500)
summary(test_impute)

completed <- complete(test_impute, )

print(colSums(is.na(completed)))

## Can try RF from here

obesity_train_final <- completed[, c(selected_predictors, "ObStatus")]
obesity_test_final <- completed[, selected_predictors]

set.seed(1)
rf_model <- train(
  ObStatus ~ .,
  data = obesity_train_final,
  method = "rf",
  trControl = train_control
)

summary(rf_model)
rf_model$results
cat("Random Forest 10-fold CV Accuracy: ", max(rf_model$results$Accuracy), ".\n")

rf_predictions <- predict(rf_model, newdata = obesity_test_final)
rf_results <- data.frame(ID = seq(1, length(rf_predictions)), ObStatus = rf_predictions)
write.csv(rf_results, "newimpute_rf_ObesitySolution.csv", row.names = FALSE)

